---
title: "ManyLabs2 -- Transparency of Methods and Results"
author: '(Corresponding coder: [Fred Hasselman](http://fredhasselman.com))'
date: "24 Feb 2015"
output:
  html_document:
    highlight: pygments
    theme: cosmo
    toc: yes
  pdf_document: default
  word_document: default
---
```{r set-options, echo=FALSE,include=FALSE}
require(knitr)
require(formatR)
require(plyr)
require(ggplot2)
require(dplyr)
options(width=100)
knitr::opts_chunk$set(cache=FALSE,comment=">",message=FALSE,echo=TRUE,warning=FALSE,tidy=TRUE,strip.white=TRUE,size="small")
```
**Open Data, Open Analyses, Valid Inferences.**
==================================================

Data generated by [ManyLabs2](https://osf.io/8cd4r) are recorded in *many files* each representing *many studies* and *many participants*, requiring *many analyses* and *many preprocessing decisions*. Therefore a data management and analysis strategy was developped based on maximal transparency and uniformity for all samples, studies and analyses.  
   
In general, the transparency of the process from measurement to inference should concern the different types of operations a researcher performs on a data set. At least 3 principles can be formulated that, if they are followed by researchers, should increase transparency and openness of research dramatically. There is a rough correspondence between a principle and different types of operations that can be performed on a data set:   
  
 1. **Principle of Equality**: All data should be treated equally by a code. That is, the code should do its job generating results while at the same time being as naive as possible to the particular facts of the study being analysed. This will reduce any chances of bias with respect to the outcomes of a certain dataset or a particular study. If it is necessary to add study specific code, the second principle should be regarded.
 2. **Principle of Transparency**: All operations that are crucial for obtaining an analysis result should be available for inspection by anyone who wishes to do so. This should be possible without the help of the auhtors that generated the code. The operations concern applying data filtering rules, constructing new variables, running an analysis and constructing graphs, tables and figures. If full transparency is not possible, the third principle should be regarded. 
 3. **Principle of Reproducibility**: The most basic requirement for analysis results is that they should be reproducable given the original code and the original data set. However, any new implementation of the same analysis strategy in a different context, or application of the code to a different dataset, e.g. a replication study, should not be problematic. That is, outcomes may differ between data sets, but this should not be attributable to any details of the code or the analysis strategy.  
    
    
    
## **1. Open Analysis: `R` as a parser of online code.**   
A procedure was developped to analyse **ManyLabs2** data in which `R` code extracts information and instructions from a Key Table and a Function Library that is openly accessible and version controlled through [GitHub](https://raw.githubusercontent.com/FredHasselman/ManyLabs2/).  This strategy is possible because **ManyLabs2** focusses on simple effects, comparing 2-3 variables, moreover, implementation was certainly fascilitated by the fact that all data were collected using the same software. The Figure is a schematic representation of the architecture.   
   
  ![ML2Setup](https://raw.githubusercontent.com/FredHasselman/ManyLabs2/master/superfluouspicture/superfluouspicture.001.png)
   
What follows is a brief summary of the steps.  
```{r data,echo=FALSE}
prefix <- "~/Dropbox/Manylabs2/TestOutput/RAW.DATA.PRIVATE/"
ML2.S1 <- readRDS(paste0(prefix,"ML2_RawData_S1.rds"))
ML2.S2 <- readRDS(paste0(prefix,"ML2_RawData_S2.rds"))
ML2.df  <- ML2.S1
ML2.primary <- list()
```
   
   
### **1.1 The Key Table**
The Key Table (`ML2masteRkey`) contains info about each analysis as well as the `R` code needed to run the analyses.
The file is available on [Google Sheets](https://docs.google.com/spreadsheets/d/1fqK3WHwFPMIjNVVvmxpMEjzUETftq_DmP5LzEhXxUHA/edit?usp=sharing). The link allows you to make comments, let us know if you see an error! Loading it directly into the `R` environment ensures you'll always have the latest version. 
   
To load the file we need and internet connection and the package `devtools` to source the `C-3PR.R` function library available on [GitHub](https://github.com/FredHasselman/toolboxR). It contains a function `get.GoogleSheet()` which will load a list stucture with 2 items directly into `R`.  
   
We're interested in the table only, which is returned the field `df` (the other object in the list is called `info`, and contains the unparsed data, the original columnnames and rownames that were retrieved).   
    
This code will source `C-3PR.R` and get the Master Key Table:
```{r github,collapse=TRUE}
require(devtools)
devtools::source_url('https://raw.githubusercontent.com/FredHasselman/manylabRs/master/R/C-3PR_ASCII.R')
devtools::source_url('https://raw.githubusercontent.com/FredHasselman/manylabRs/master/R/getData.R')
devtools::source_url('https://raw.githubusercontent.com/FredHasselman/manylabRs/master/R/fRedsRutils.R')

ML2.key <- get.GoogleSheet(data='ML2masteRkey')$df
```
   
You'll see a lot of functions pop up in your environment. Don't worry, you'll need just 1 or 2 functions to reproduce the analyses as they were published. The variable `ML2.key` is a data table containing the Google Sheet. It is the `dplyr` version of a data frame, which gives a neat summary in the concole if you just type its name:   
```{r, keytable0,collapse=TRUE}
ML2.key
```
  
  
For each **Analysis** to be performed there is a row in the Key Table containing `R` code with all the information that is needed to run the analysis:   
   
* The name of the study and the analysis (`study.analysis`)   
* Which variables in the data files belong to variables to be used in the analysis (`study.vars` and `study.vars.labels`)    
* Which cases and sites to include in the analysis, based on selection criteria (`study.cases.include` and `study.sites.include`)   
* The `R` code to run the analysis (`stat.test`).  
* Code to read relevant statistics from the output structure of the analyses   
  
  
### **1.2 Example: Huang, Tse & Cho, 2014, Study 1a**
In study 1 of slate 1, analysis 2, participants clicked on a map to indicate where a fictional character they had read about would live. There were 2 conditions, the character could have been described as high SES, or a low SES individual (test the study here: https://ufl.qualtrics.com/SE/?SID=SV_cNiQVmpTU8Xd1uB).  
   
The Y coordinate of the mouseclick (North or South) is the dependent variable in a linear model which tests the interaction between the Condition (HighSES vs. LowSES) and an answer to the individual difference measures at the end of the slate where participants reported the name of their home town and answered: Where do wealthier people live in your home town? Response options: north side (1), south side (2), or neither (3).   
   
There were several inclusion rules: A valid value for Y and for the survey answer and a Y click *inside* the map.
This is the inclusion rule that is listed in the `masteRkey` table:
```{r, keytable1,collapse=TRUE}
ML2.key[2,'study.cases.include']
```
The command returned tells the script to check the following condition for `each` variable `X` listed in `Condition`: `X > 0`. 
   
Which variables are grouped under `Condition`?   
```{r, keytable2,collapse=TRUE}
ML2.key[1,'study.vars']
ML2.key[1,'study.vars.labels']
```
   
   
For most studies, the information in these fields of the KeyTable is sufficient to create variables and feed those to the command in `stat.test` which will run the statistical analysis:
```{r, keytable4,collapse=TRUE}
ML2.key[1,'stat.test']
```
   
The `R` code will just organise the data as indicated by the list structures in the KeyTable. This is achieved by parsing and evaluating the code in the fields. For example, evaluating the `study.vars` field creates a named list structure with a slot `High` and `Low` and this is just what the `t.test` command above needs for its `x` and `y` input.
```{r, parse}
eval(parse(text=ML2.key[1,'study.vars']))
```
  
  
### **1.2 The Function Library**
Some studies require a transformation of variables that is more complex. For those studies the action takes place in special purpose variable functions `varfun.ABC.N`. Those functions are in a function library `ML2_funlib.R` that is also available on [GitHub](https://github.com/FredHasselman/ManyLabs2). To source this file directly from Github, we need the `devtools` library, the `url` is the same as for the KeyTable.   
```{r funlib}
devtools::source_url("https://raw.githubusercontent.com/FredHasselman/manylabRs/master/R/ML2_variable_functions.R")
```
   
All analyses have a `varfun.ABC.N`, but most just return a convenient list to pass to the analysis. The first study was straightforward, to see its special function call it without any arguments `varfun.Huang.1`:
```{r, parsevarfun1,tidy.opts=list(comment=TRUE)}
eval(parse(text=ML2.key[1,'stat.vars']))
```
This function just returns whatever is passed as `ML2.sr` (a list object with clean source data required for analysis, see Section 3).   
More complex is for example Study 3, analysis 4: `varfun.Alter.1`. The study used a number of syllogisms and needs to analyse items of intermediate difficulty. One of the planned ManyLabs2 analyses would be different from the original and focus on the percentage syllogisms answered correctly in the sample to select the target items (between .25 and .75 correct). The script translates the recorded responses into the correct values and variables:
```{r, parsevarfun2,tidy.opts=list(comment=TRUE)}
eval(parse(text=ML2.key[4,'stat.vars']))
```
    
    
    
--------   
     
     
     
### **1.3 Open Science Summary**       
```
Open Science Summary:
   
i.   All analysis steps are available for peer scrutiny in 2 files that are openly accessible on GitHub.
   
ii.  The Key Table contains info on the data structure, manipulation and the R code used to run the statistical analysis.
   
iii. The Function Library can be queried to reveal study-specific pre-processing steps.
   
iv.  The GitHub and R platform provide a means for the scientific community to:
     - Ask questions about the files in an open issue tracking system   
     - Track the version history of files
     - Contribute by suggesting corrections or amendments
     - Work with the most recent data and analysis scripts by loading the online files into the local R environment.
```  
    
-------- 
    
    
    
## **2. Open Data Censoring: Selecting variables and cases for analysis**
  
Data censoring is the most important step between measurement and statistical inference. In general, all data that were reliably measured, should be used in an analysis. An outlier only exists relative to an assumption about the true probability distribution function of an observable. If a certain type of analysis cannot handle outliers in data, that analysis is inadequate and should be removed from the analytic toolbox, not the carefully measured data points.   
   
   
### **2.1 Generating Filter Chains**
The **ManyLabs2** function library `ML2_funlib.R` contains the function `inIT()`, which will load, and if necessary, install packages on your system if they are not present (`unIT()` will unload, and if requested, uninstall packages). To prepare for what follows, load a number of packages:
```{r, init}
devtools::source_url("https://raw.githubusercontent.com/FredHasselman/manylabRs/master/R/inIT.R")
in.IT(c("plyr","dplyr","xlsx","metafor","lattice"))
```
The Key Table lists rules for cases to be included in the field `study.cases.include`, for sites to be included in the field `study.sites.include` and in the field `stat.params` through the logical flag `censorNA`.   
For Analysis 2 (`Huang.2`), the settings are as follows:
```{r, keytable5,collapse=TRUE}
ML2.key[2,'study.cases.include']
ML2.key[2,'study.sites.include']
ML2.key[2,'stat.params']
```  
The next step is to select the correct slate for the data set `ML2.df` (already pre-loaded for this example), and gather information about the analysis to run using the function `get.info()`. Below, a list object  `ML2.in` containing all the information for this study from the Key Table is obtained. The structure ispassed to the function `get.chain()` which will produce a sequence of pre-processing steps.   
```{r, selecdf1}
# Get info to create a dataset for the current study
ML2.in  <- get.info(ML2.key[2,],colnames(ML2.df))
ML2.id  <- get.chain(ML2.in)
ML2.id$vars
```
The *chain* operator `%>%` (or *pipe*, based on package `magrittr`), is one of several very useful commands for handling datasets provided by the package `dplyr`. For more information see the [`dplyr` CRAN vignette](http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html). You can just read '*...and then...*'.    
   
In the present case the instruction says:  
   - Take the data in `Condition$High` *...and then...*   
   - filter cases who have a value for `huan1.1_Y1` greater than 0 *...&...*   
   - do not have a missing value on `huan1.1_Y1` *...&...*  
   - have `homewealth` greater than 0 *...&...*   
   - do not have a missing value on `homewealth` *...&...*   
   - have a value for `homewealth` between and including 1 and 2  
   
There is redundancy in the chain, but without any adverse effects. For example `homewealth` has to be > 0 *and* between 1 and 2.  
The script just replaced the `X` in the string read from the field `study.cases.include` with variable names that belong to `Condition` according tp the `study.vars.labels` field:
```{r, chain,collapse=TRUE}
ML2.key[2,'study.cases.include']
ML2.key[2,'study.vars.labels']
```
The title of the fields in `study.cases.include` indicates whether the rule in the subsequent field(s) applies to `each` (OR, `|`) variable seperately, or to the `complete` (AND, `&`) set of variables.   
    
The list `ML2.id` also contains the rule for selecting the variables from the full dataset. It is based on `ML2.in` and the field `study.sites.include`:
```{r, chaindf,collapse=TRUE}
ML2.in$id.vars
ML2.key[2,'study.sites.include']
ML2.id$df
```
This instruction says:  
   - Take the data in `ML2.df` *...and then...*   
   - select the variables with this column index *...and then...*  
   - select the cases for which the `.id` variable is a character (= 'all' cases, because `.id` is the filename of the site dataset)   
   
### **2.2 Applying Filter Chains and Special Functions**

Get the filtered data set using the function `get.sourceData()`.
```{r, getdata1}
ML2.sr <- get.sourceData(ML2.id,ML2.df,ML2.in)
ML2.sr
```

Finally, the data set to pass to the analysis is generated by the code in the `stat.vars` field. In most cases, this will not change anything. For the present study variables were reorganised such that a factor `SESvignette` distinguishes conditions, a factor `SEShomewealth` indicates wether wealthy people live in the north or south in the city of residence of the participant. The dependent variable `MapYcLick` is the Y coordinate of a mouse click on a map (test the experiment here).
```{r, getdata2,collapse=TRUE}
ML2.var <- eval(parse(text=paste0(ML2.key[2,'stat.vars'],'(ML2.sr)',collapse="")))
summary(ML2.var)
```
This data structure was computed by the special function `varfun.Huang.2`:
```{r, getdata3}
eval(parse(text=ML2.key[2,'stat.vars']))
```
As can be seen in the code, this transformation merely concerns a rearrangement of the data and the generation of indicator variables. This is the scenario for most studies in **ManyLabs2**.    
    
    
### **2.3 Save Pre- and Post- Filter Chain Data**
The Function Library contains a function `save.ML2`. It takes a list object in which for each analysis `s`, the data structures generated by `get.info()`, `get.chain()`, `get.soureData()` and the study specific `varfun.`, function, as well as the results:
```{r, savedata0,eval=FALSE}
ML2.data[[s]] <- list(stat.analysis.name  = ML2.key[s,'study.analysis'],
                      stat.info           = ML2.in,
                      stat.data.cleanchain= ML2.id,
                      stat.data.raw       = ML2.df,
                      stat.data.precleaned= ML2.sr,
                      stat.data.analysed  = ML2.var,
                      stat.test.result    = stat.test)
```
By default the function will save an Excel workbook for each study, with sheets representing the list items and a single `R` data file containing the list object.
```{r, savedata1,eval=FALSE}
save.ML2(ML2.data)
```
This way, for each analysis, the pre- and post filter data are available in one workbook, together with the results.
     
     
 --------   
     
     
     
### **2.4 Open Science Summary:**       
```   
i.   All data manipulation steps are available for peer scrutiny in a Filter Chain generated by the code.    
   
ii.  The Filter Chain is readable and interpretable in common language: "Take X and then Select Y based on..."    
   
iii. The data from -before- and -after- applying filtering rules, including the results are available in an `R` file and spreadsheets.
   
```  
      
 --------   
     
       
     
## **3. Open Results**
The final step is to run the analysis listed in the Key Table field `stat.test` using global setting listed in `stat.params`.   
### 3.1 Run the Analysis
```{r, getresults1}
stat.params <- ML2.in$stat.params
stat.params
```
The `alternative greater` option is just to remind us the F-distribution is not symmetrical and in the present context a directional hypothesis of `F > 1` is the only valid statistical hypothesis.   
```{r, getresults2}
ML2.key[2,'stat.test']
```
Looking for an interaction between between the SESvignette (Hig or Low SES) HomeWealth (North or South) and where a participant clicked on the map (Y-coordinate).
```{r, getresults3}
stat.test   <- with(ML2.var,eval(parse(text=ML2.key[2,'stat.test'])))
stat.test
```
    
    
### **3.1 Store Analysis Information and Results**
To store the results from the ANOVA table, we need the information in `stat.ncp` and `stat.df`:
```{r, getresults4,tidy=FALSE}
eval(parse(text=ML2.key[2,'stat.ncp']))
eval(parse(text=ML2.key[2,'stat.df']))
```
   
These values can be used to generate Effect Size Confidence Intervals, all info is collected in a list structure: 
```{r, getresults5,tidy=FALSE}
ML2.primary[[2]] <- list(analysis.id   = c(ML2.key[2,'study.id'],ML2.key[2,'study.slate'],ML2.key[2,'study.name']),
                           analysis.name = ML2.key[2,'study.analysis'],
                           stat.varfun   = ML2.key[2,'stat.vars'],
                           stat.type     = ML2.key[2,'stat.type'],
                           stat.ncp      = eval(parse(text=ML2.key[2,'stat.ncp'])),
                           stat.df       = eval(parse(text=ML2.key[2,'stat.df'])),
                           stat.N        = ML2.var$N,
                           stat.info     = ML2.in,
                           analysis.extra = list(stat.test = stat.test))  
ML2.primary[[2]] 
```
     
     
 --------   
     
     
     
### **3.2 Open Science Summary:**       
```   
i.   All commands and settings used to run the statistical analyses are available for peer scrutiny in the openly accessible Key Table.    
   
ii. The results are presented in such a way that they can be used for further evaluation.
```  
    
    
 --------   
     
     
     
